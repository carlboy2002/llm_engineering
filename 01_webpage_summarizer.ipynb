{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53211323-6a09-452a-b471-98e22d92bfc2",
   "metadata": {},
   "source": [
    "# üåê WebPage Summarizer\n",
    "---\n",
    "- üåç **Task:** Summarizing webpage content using AI.  \n",
    "- üß† **Model:** OpenAI's ``gpt-4o-mini`` and ``llama3.2`` for text summarization.  \n",
    "- üïµÔ∏è‚Äç‚ôÇÔ∏è **Data Extraction:** Selenium for handling both static and JavaScript-rendered websites.  \n",
    "- üìå **Output Format:** Markdown-formatted summaries.  \n",
    "- üîó **Scope:** Processes only the given webpage URL (not the entire site).  \n",
    "- üöÄ **Tools:** Python, Requests, Selenium, BeautifulSoup, OpenAI API, Ollama. \n",
    "- üßë‚Äçüíª **Skill Level:** Beginner.\n",
    "\n",
    "üõ†Ô∏è Requirements\n",
    "- ‚öôÔ∏è Hardware: ‚úÖ CPU is sufficient ‚Äî no GPU required\n",
    "- üîë OpenAI API Key (for GPT model)\n",
    "- Install Ollama and pull llama3.2:3b or another lightweight model\n",
    "- Google Chrome browser installed\n",
    "\n",
    "**‚ú® This script handles both JavaScript and non-JavaScript websites using Selenium with Chrome WebDriver for reliable content extraction from modern web applications.**\n",
    "\n",
    "Let's get started and automate website summarization! üöÄ\n",
    "\n",
    "![](https://github.com/lisekarimi/lexo/blob/main/assets/01_basic_llm_project.jpg?raw=true)\n",
    "\n",
    "---\n",
    "üì¢ Find more LLM notebooks on my [GitHub repository](https://github.com/lisekarimi/lexo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70aa4b0",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf2fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Collecting typing_extensions~=4.14.0 (from selenium)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Collecting sortedcontainers (from trio~=0.30.0->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in e:\\anaconda\\envs\\llms\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from trio~=0.30.0->selenium) (2.0.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in e:\\anaconda\\envs\\llms\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in e:\\anaconda\\envs\\llms\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in e:\\anaconda\\envs\\llms\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: pycparser in e:\\anaconda\\envs\\llms\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\anaconda\\envs\\llms\\lib\\site-packages (from requests->webdriver-manager) (3.4.3)\n",
      "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 7.3/9.6 MB 41.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 39.7 MB/s  0:00:00\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, typing_extensions, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [wsproto]\n",
      "  Attempting uninstall: typing_extensions\n",
      "   ----- ---------------------------------- 1/8 [wsproto]\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "   ----- ---------------------------------- 1/8 [wsproto]\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "   ----- ---------------------------------- 1/8 [wsproto]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [typing_extensions]\n",
      "   -------------------- ------------------- 4/8 [webdriver-manager]\n",
      "   -------------------- ------------------- 4/8 [webdriver-manager]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [trio]\n",
      "   ------------------------------ --------- 6/8 [trio-websocket]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ---------------------------------------- 8/8 [selenium]\n",
      "\n",
      "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 webdriver-manager-4.0.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcf1d9d-c540-4900-b14e-ad36a28fc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# System & Environment\n",
    "# ===========================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ===========================\n",
    "# Web Scraping\n",
    "# ===========================\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ===========================\n",
    "# AI-related\n",
    "# ===========================\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20642b",
   "metadata": {},
   "source": [
    "## üîê Model Configuration & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8598c299-05ca-492e-b085-6bcc2f7dda0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "   raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully!\")\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8098defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OPENAI = \"gpt-4o-mini\"\n",
    "MODEL_OLLAMA = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1d83f",
   "metadata": {},
   "source": [
    "## üåê Web Scraping Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fe5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteCrawler:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.title = \"\"\n",
    "        self.text = \"\"\n",
    "        self.scrape()\n",
    "\n",
    "    def scrape(self):\n",
    "        try:\n",
    "            # Chrome options\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "            chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "            # Try to find Chrome\n",
    "            chrome_paths = [\n",
    "                r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "                r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "                r\"C:\\Users\\{}\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe\".format(os.getenv('USERNAME')),\n",
    "            ]\n",
    "\n",
    "            chrome_binary = None\n",
    "            for path in chrome_paths:\n",
    "                if os.path.exists(path):\n",
    "                    chrome_binary = path\n",
    "                    break\n",
    "\n",
    "            if chrome_binary:\n",
    "                chrome_options.binary_location = chrome_binary\n",
    "\n",
    "            # Create driver\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.set_page_load_timeout(30)\n",
    "\n",
    "            print(f\"üîç Loading: {self.url}\")\n",
    "            driver.get(self.url)\n",
    "\n",
    "            # Wait for page to load\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Try to wait for main content\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"main\"))\n",
    "                )\n",
    "            except Exception:\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass  # Continue anyway\n",
    "\n",
    "            # Get title and page source\n",
    "            self.title = driver.title\n",
    "            page_source = driver.page_source\n",
    "            driver.quit()\n",
    "\n",
    "            print(f\"‚úÖ Page loaded: {self.title}\")\n",
    "\n",
    "            # Parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Remove unwanted elements\n",
    "            for element in soup([\"script\", \"style\", \"img\", \"input\", \"button\", \"nav\", \"footer\", \"header\"]):\n",
    "                element.decompose()\n",
    "\n",
    "            # Get main content\n",
    "            main = soup.find('main') or soup.find('article') or soup.find('.content') or soup.find('body')\n",
    "            if main:\n",
    "                self.text = main.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                self.text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            # Clean up text\n",
    "            lines = [line.strip() for line in self.text.split('\\n') if line.strip() and len(line.strip()) > 2]\n",
    "            self.text = '\\n'.join(lines[:200])  # Limit to first 200 lines\n",
    "\n",
    "            print(f\"üìÑ Extracted {len(self.text)} characters\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error occurred: {e}\")\n",
    "            self.title = \"Error occurred\"\n",
    "            self.text = \"Could not scrape website content\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727feff",
   "metadata": {},
   "source": [
    "## üß† Prompt Engineering & Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e3a673-a8a1-4101-a441-3816f7ab9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86bb80f9-9e7c-4825-985f-9b83fe50839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89998b18-77aa-4aaf-a137-f0d078d61f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde36d4f",
   "metadata": {},
   "source": [
    "## üìù Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5636affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading: https://openai.com\n",
      "‚úÖ Page loaded: OpenAI\n",
      "üìÑ Extracted 3417 characters\n",
      "ü§ñ Creating summary...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# OpenAI Website Summary\n",
       "\n",
       "OpenAI's website serves as a hub for AI-driven products and services, featuring various tools and resources for users. Key components include:\n",
       "\n",
       "## Features\n",
       "- **ChatGPT**: A versatile AI tool assisting with diverse tasks ranging from travel planning and language translation to coding and creative writing.\n",
       "- **GPT-5**: The latest model introduced, noted as the smartest and fastest yet, enhancing various applications including creative and medical research.\n",
       "\n",
       "## News and Announcements\n",
       "- **GPT-5 Release**: Positioned as OpenAI's most advanced model, aimed at making AI more useful across different domains.\n",
       "- **Stargate Updates**: Partnership with Oracle and SoftBank to expand AI data center capabilities.\n",
       "- **New Collaborations**: Strategic partnerships with companies like NVIDIA and SAP to enhance AI infrastructure and services.\n",
       "\n",
       "## Recent Publications\n",
       "- Focused on real-world applications of their AI models and ongoing enhancements in safety, performance measurement, and reducing biases in AI.\n",
       "\n",
       "The website emphasizes innovative AI applications, partnerships, and updates for effective deployment of AI solutions in various fields."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_gpt(url):\n",
    "    \"\"\"Scrape website and summarize with GPT\"\"\"\n",
    "    site = WebsiteCrawler(url)\n",
    "\n",
    "    if \"Error occurred\" in site.title or len(site.text) < 50:\n",
    "        print(f\"‚ùå Failed to scrape meaningful content from {url}\")\n",
    "        return\n",
    "\n",
    "    print(\"ü§ñ Creating summary...\")\n",
    "\n",
    "    # Create summary\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL_OPENAI,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_for(site)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    web_summary = response.choices[0].message.content\n",
    "    display(Markdown(web_summary))\n",
    "\n",
    "summarize_gpt('https://openai.com')\n",
    "# summarize_gpt('https://stripe.com')\n",
    "# summarize_gpt('https://vercel.com')\n",
    "# summarize_gpt('https://react.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b9a8f8-0c1c-40c8-a4b3-e8e1fcd29df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading: https://github.com\n",
      "‚úÖ Page loaded: GitHub ¬∑ Build and ship software on a single, collaborative platform ¬∑ GitHub\n",
      "üìÑ Extracted 4757 characters\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# GitHub\n",
       "### A Collaborative Platform for Building and Shipping Software\n",
       "\n",
       "GitHub is a leading platform for developers to build, ship, and manage software projects collaboratively. The company features various tools and services, including GitHub Copilot, an AI-powered developer platform that assists with coding, security, and more.\n",
       "\n",
       "## Features\n",
       "\n",
       "- **GitHub Copilot**: A chat-based code editor that provides AI-powered assistance with writing, reviewing, and refactoring code.\n",
       "- **Dependabot**: A tool for automating dependency updates to ensure software is secure and up-to-date.\n",
       "- **GitHub Actions**: A comprehensive platform for managing CI/CD (Continuous Integration and Continuous Deployment) pipelines.\n",
       "- **GitHub Codespaces**: A cloud-based development environment that allows users to start building software immediately.\n",
       "\n",
       "## Benefits\n",
       "\n",
       "- **Increased Productivity**: GitHub Copilot helps developers work 55% faster, with features like code completion, chat, and more.\n",
       "- **Improved Security**: The platform offers automated vulnerability fixes, security campaigns, and secret scanning for detecting and preventing leaked secrets across organizations.\n",
       "- **Enhanced Collaboration**: GitHub provides a single, collaborative platform for managing projects, issues, pull requests, and discussions.\n",
       "\n",
       "## Customer Stories\n",
       "\n",
       "- Duolingo boosts developer speed by 25% with GitHub Copilot.\n",
       "- Mercedes-Benz standardizes source code and automates onboarding using GitHub.\n",
       "- Mercado Libre cuts coding time by 50% using GitHub.\n",
       "\n",
       "### News and Announcements\n",
       "\n",
       "* The AI wave continues to grow on software development teams, as stated in a survey (2024).\n",
       "\n",
       "Note: This summary focuses on the core features and benefits of GitHub and its associated products, while ignoring navigation-related text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_ollama(url):\n",
    "    website = WebsiteCrawler(url)\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_OLLAMA,\n",
    "        messages=messages_for(website))\n",
    "    display(Markdown(response['message']['content']))  # Generate and display output\n",
    "\n",
    "summarize_ollama('https://github.com')\n",
    "# summarize_ollama('https://nextjs.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b65a29-5e5d-4b15-9e4e-d4757635a893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
